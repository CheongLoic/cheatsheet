{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- host_is_superhost: string (nullable = true)\n |-- cancellation_policy: string (nullable = true)\n |-- instant_bookable: string (nullable = true)\n |-- host_total_listings_count: integer (nullable = true)\n |-- neighbourhood_cleansed: string (nullable = true)\n |-- latitude: double (nullable = true)\n |-- longitude: double (nullable = true)\n |-- property_type: string (nullable = true)\n |-- room_type: string (nullable = true)\n |-- accommodates: integer (nullable = true)\n |-- bathrooms: double (nullable = true)\n |-- bedrooms: integer (nullable = true)\n |-- beds: integer (nullable = true)\n |-- bed_type: string (nullable = true)\n |-- minimum_nights: integer (nullable = true)\n |-- number_of_reviews: integer (nullable = true)\n |-- review_scores_rating: integer (nullable = true)\n |-- review_scores_accuracy: integer (nullable = true)\n |-- review_scores_cleanliness: integer (nullable = true)\n |-- review_scores_checkin: integer (nullable = true)\n |-- review_scores_communication: integer (nullable = true)\n |-- review_scores_location: integer (nullable = true)\n |-- review_scores_value: integer (nullable = true)\n |-- price: integer (nullable = true)\n |-- bedrooms_na: integer (nullable = true)\n |-- bathrooms_na: integer (nullable = true)\n |-- beds_na: integer (nullable = true)\n |-- review_scores_rating_na: integer (nullable = true)\n |-- review_scores_accuracy_na: integer (nullable = true)\n |-- review_scores_cleanliness_na: integer (nullable = true)\n |-- review_scores_checkin_na: integer (nullable = true)\n |-- review_scores_communication_na: integer (nullable = true)\n |-- review_scores_location_na: integer (nullable = true)\n |-- review_scores_value_na: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "airbnbDF = (spark.read.format('csv')\n",
    "                    .option('header', 'true')\n",
    "                    .option('inferSchema', 'true')\n",
    "                    .option('sep', ',')\n",
    "                    .load('hdfs:///education/ece/big-data/2020/fall/bda/resources/lab5/airbnb_clean.csv'))\n",
    "\n",
    "airbnbDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of rows in train DF: 802\nNo. of rows in test DF: 198\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "# Split dataset to train and test\n",
    "\n",
    "trainDF, testDF = airbnbDF.randomSplit([.8, .2], seed=123)\n",
    "print 'No. of rows in train DF:', trainDF.count()\n",
    "print 'No. of rows in test DF:', testDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n|price|accommodates|\n+-----+------------+\n|  130|           3|\n|  250|           4|\n|  405|           6|\n|  185|           2|\n|  135|           2|\n|  150|           3|\n|  155|           4|\n|  160|           1|\n| 2281|           2|\n|  297|           2|\n|   81|           4|\n|   76|           5|\n|  155|           2|\n|  150|           2|\n|   75|           2|\n|  162|           2|\n|  200|           3|\n|  115|           2|\n|  125|           2|\n|  159|           3|\n+-----+------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "# We will try to predict the price in function of how many people can stay at the property\n",
    "\n",
    "trainDF.select(\"price\", \"accommodates\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n|summary|             price|      accommodates|\n+-------+------------------+------------------+\n|  count|               802|               802|\n|   mean|213.15960099750623|3.2768079800498753|\n| stddev|203.85681852899708| 1.990786715309765|\n|    min|                29|                 1|\n|    25%|               100|                 2|\n|    50%|               150|                 2|\n|    75%|               249|                 4|\n|    max|              2281|                15|\n+-------+------------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "# We can first summarize the DF to see if it's really clean\n",
    "\n",
    "trainDF.select(\"price\", \"accommodates\").summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "%pyspark\n",
    "# Define linear regression estimator and fit the model\n",
    "# Why doesn't it work?\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(featuresCol=\"accommodates\", labelCol=\"price\")\n",
    "#lrModel = lr.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n|accommodates|features|\n+------------+--------+\n|           3|   [3.0]|\n|           4|   [4.0]|\n|           6|   [6.0]|\n|           2|   [2.0]|\n|           2|   [2.0]|\n|           3|   [3.0]|\n|           4|   [4.0]|\n|           1|   [1.0]|\n|           2|   [2.0]|\n|           2|   [2.0]|\n|           4|   [4.0]|\n|           5|   [5.0]|\n|           2|   [2.0]|\n|           2|   [2.0]|\n|           2|   [2.0]|\n|           2|   [2.0]|\n|           3|   [3.0]|\n|           2|   [2.0]|\n|           2|   [2.0]|\n|           3|   [3.0]|\n+------------+--------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "# Vectorize the column 'accommodates'\n",
    "# Pay attention to .transform()\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vecAssembler = VectorAssembler(inputCols=[\"accommodates\"], outputCol=\"features\")\n",
    "vecTrainDF = vecAssembler.transform(trainDF)\n",
    "vecTrainDF.select([\"accommodates\", \"features\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+---------+\n|accommodates|bedrooms| features|\n+------------+--------+---------+\n|           3|       1|[3.0,1.0]|\n|           4|       2|[4.0,2.0]|\n|           6|       3|[6.0,3.0]|\n|           2|       1|[2.0,1.0]|\n|           2|       1|[2.0,1.0]|\n|           3|       1|[3.0,1.0]|\n|           4|       1|[4.0,1.0]|\n|           1|       1|[1.0,1.0]|\n|           2|       1|[2.0,1.0]|\n|           2|       1|[2.0,1.0]|\n|           4|       1|[4.0,1.0]|\n|           5|       2|[5.0,2.0]|\n|           2|       1|[2.0,1.0]|\n|           2|       1|[2.0,1.0]|\n|           2|       1|[2.0,1.0]|\n|           2|       1|[2.0,1.0]|\n|           3|       1|[3.0,1.0]|\n|           2|       1|[2.0,1.0]|\n|           2|       1|[2.0,1.0]|\n|           3|       1|[3.0,1.0]|\n+------------+--------+---------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "# Let's try with two input columns\n",
    "\n",
    "vecAssembler_2 = VectorAssembler(inputCols=[\"accommodates\", \"bedrooms\"], outputCol=\"features\")\n",
    "vecTrainDF_2 = vecAssembler_2.transform(trainDF)\n",
    "vecTrainDF_2.select([\"accommodates\", \"bedrooms\", \"features\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)', 'elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)', 'epsilon: The shape parameter to control the amount of robustness. Must be > 1.0. Only valid when loss is huber (default: 1.35)', 'featuresCol: features column name. (default: features, current: features)', 'fitIntercept: whether to fit an intercept term. (default: True)', 'labelCol: label column name. (default: label, current: price)', 'loss: The loss function to be optimized. Supported options: squaredError, huber. (default: squaredError)', 'maxIter: max number of iterations (>= 0). (default: 100)', 'predictionCol: prediction column name. (default: prediction)', 'regParam: regularization parameter (>= 0). (default: 0.0)', 'solver: The solver algorithm for optimization. Supported options: auto, normal, l-bfgs. (default: auto)', 'standardization: whether to standardize the training features before fitting the model. (default: True)', 'tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)', 'weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)']\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "# Which parameters can the estimator take?\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"price\")\n",
    "lr.explainParams().split('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression_4d59b3f133110b6f7a88\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "# Fit the estimator to the data\n",
    "\n",
    "lrModel = lr.fit(vecTrainDF)\n",
    "lrModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 62.627665173598565 * x + 7.94076798477\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "# Print the coefficients of the model\n",
    "\n",
    "k = lrModel.coefficients[0]\n",
    "n = lrModel.intercept\n",
    "\n",
    "print 'y =', k, '* x +', n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. mertics: [0.3780198471330412]\nBest model slope: 62.627665173598565\nBest model intercept: 7.94076798477\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "# Perform the cross validation (and optionally grid search)\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Define the params for cv (and grid search: commented line)\n",
    "params = (ParamGridBuilder()\n",
    "          #.addGrid(lr.elasticNetParam, [0.0, 1.0])\n",
    "          .build())\n",
    "evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "cv = CrossValidator(estimator=lr, evaluator=evaluator, estimatorParamMaps=params, numFolds=10, seed=11)\n",
    "cvModel = cv.fit(vecTrainDF)\n",
    "\n",
    "print 'Avg. metrics:', cvModel.avgMetrics\n",
    "print 'Best model slope:', cvModel.bestModel.coefficients[0]\n",
    "print 'Best model intercept:', cvModel.bestModel.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+-----+------------------+\n|accommodates|features|price|        prediction|\n+------------+--------+-----+------------------+\n|           4|   [4.0]|  350|  258.451428679161|\n|           2|   [2.0]|  250|133.19609833196387|\n|           6|   [6.0]|  275|383.70675902635816|\n|           2|   [2.0]|  115|133.19609833196387|\n|           8|   [8.0]|  145| 508.9620893735553|\n|           2|   [2.0]|  135|133.19609833196387|\n|           1|   [1.0]|   67| 70.56843315836531|\n|           2|   [2.0]|  141|133.19609833196387|\n|           2|   [2.0]|  157|133.19609833196387|\n|           2|   [2.0]|   50|133.19609833196387|\n|           2|   [2.0]|   45|133.19609833196387|\n|           1|   [1.0]|   45| 70.56843315836531|\n|           2|   [2.0]|  165|133.19609833196387|\n|           4|   [4.0]|  195|  258.451428679161|\n|           3|   [3.0]|  143|195.82376350556245|\n|           3|   [3.0]|  150|195.82376350556245|\n|           6|   [6.0]|  425|383.70675902635816|\n|           3|   [3.0]|  126|195.82376350556245|\n|           4|   [4.0]|  138|  258.451428679161|\n|           4|   [4.0]|  418|  258.451428679161|\n+------------+--------+-----+------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "# Apply the model to the test set. But, first we need to process the test data the same way we processed the train data.\n",
    "\n",
    "vecTestDF = vecAssembler.transform(testDF)\n",
    "predDF = cvModel.transform(vecTestDF)\n",
    "\n",
    "predDF.select(\"accommodates\", \"features\", \"price\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+-----+------------------+\n|accommodates|features|price|        prediction|\n+------------+--------+-----+------------------+\n|           4|   [4.0]|  350|  258.451428679161|\n|           2|   [2.0]|  250|133.19609833196387|\n|           6|   [6.0]|  275|383.70675902635816|\n|           2|   [2.0]|  115|133.19609833196387|\n|           8|   [8.0]|  145| 508.9620893735553|\n|           2|   [2.0]|  135|133.19609833196387|\n|           1|   [1.0]|   67| 70.56843315836531|\n|           2|   [2.0]|  141|133.19609833196387|\n|           2|   [2.0]|  157|133.19609833196387|\n|           2|   [2.0]|   50|133.19609833196387|\n|           2|   [2.0]|   45|133.19609833196387|\n|           1|   [1.0]|   45| 70.56843315836531|\n|           2|   [2.0]|  165|133.19609833196387|\n|           4|   [4.0]|  195|  258.451428679161|\n|           3|   [3.0]|  143|195.82376350556245|\n|           3|   [3.0]|  150|195.82376350556245|\n|           6|   [6.0]|  425|383.70675902635816|\n|           3|   [3.0]|  126|195.82376350556245|\n|           4|   [4.0]|  138|  258.451428679161|\n|           4|   [4.0]|  418|  258.451428679161|\n+------------+--------+-----+------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "# ALTERNATIVE\n",
    "# Create a pipeline from all of the stages before (data preparation + model definition. Don't put cv in the pipeline.)\n",
    "# Apply the pipeline to test frame\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "vecAssembler = VectorAssembler(inputCols=[\"accommodates\"], outputCol=\"features\")\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"price\")\n",
    "pipeline = Pipeline(stages=[vecAssembler, lr])\n",
    "\n",
    "pipelineModel = pipeline.fit(trainDF)\n",
    "\n",
    "predDF = pipelineModel.transform(testDF)\n",
    "predDF.select(\"accommodates\", \"features\", \"price\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 of the linear model is: 0.427506233551\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "# How good is the model?\n",
    "# RegressionEvaluator will compute the differences between the predicted price and the real one and calculate the metric of overall model quality (R2)\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "regressionEvaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"price\", metricName=\"r2\")\n",
    "\n",
    "r2 = regressionEvaluator.evaluate(predDF)\n",
    "print 'R2 of the linear model is:', r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab: Random Forest Regression\n",
    "Build and evaluate a Random Forest model the same way we did with Linear Regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "%pyspark\n",
    "# Spit data to train and test set. Use the same split and seed as for linear regression, so we can compare the models at the end.\n",
    "\n",
    "trainDF, testDF = airbnbDF.randomSplit([.8, .2], seed=123)\n",
    "print 'No. of rows in train DF:', trainDF.count()\n",
    "print 'No. of rows in test DF:', testDF.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "%pyspark\n",
    "# Select only three atributes, since we didn't have time to look at the one-hot encoding - accommodates, bathrooms, review_scores_rating. \n",
    "# Predict the price as before\n",
    "# Name the dataframe trainDF \n",
    "\n",
    "cols =  ['accommodates', 'bathrooms', 'review_scores_rating', 'price']\n",
    "trainDF = trainDF.select(cols)\n",
    "trainDF.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+\n|accommodates|       features|\n+------------+---------------+\n|           3| [3.0,1.0,88.0]|\n|           4| [4.0,2.0,74.0]|\n|           6|[6.0,2.5,100.0]|\n|           2| [2.0,1.0,96.0]|\n|           2| [2.0,1.0,95.0]|\n|           3| [3.0,1.0,92.0]|\n|           4| [4.0,1.0,99.0]|\n|           1| [1.0,1.0,80.0]|\n|           2| [2.0,1.0,99.0]|\n|           2| [2.0,1.0,97.0]|\n|           4| [4.0,1.0,98.0]|\n|           5| [5.0,1.0,93.0]|\n|           2| [2.0,1.0,99.0]|\n|           2| [2.0,1.0,93.0]|\n|           2| [2.0,2.0,91.0]|\n|           2| [2.0,1.0,98.0]|\n|           3| [3.0,1.0,76.0]|\n|           2| [2.0,1.0,98.0]|\n|           2| [2.0,1.0,89.0]|\n|           3| [3.0,1.0,94.0]|\n+------------+---------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "# Vectorize the attribute (feature) columns with VectorAssembler\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "input_cols =  ['accommodates', 'bathrooms', 'review_scores_rating']\n",
    "vecAssembler = VectorAssembler(inputCols=input_cols, outputCol=\"features\")\n",
    "\n",
    "vecTrainDF = vecAssembler.transform(trainDF)\n",
    "vecTrainDF.select([\"accommodates\", \"features\"]).show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval. (default: False)', 'checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)', 'featureSubsetStrategy: The number of features to consider for splits at each tree node. Supported options: auto, all, onethird, sqrt, log2, (0.0-1.0], [1-n]. (default: auto)', 'featuresCol: features column name. (default: features)', 'impurity: Criterion used for information gain calculation (case-insensitive). Supported options: variance (default: variance)', 'labelCol: label column name. (default: label, current: price)', 'maxBins: Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature. (default: 32, current: 40)', 'maxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. (default: 5)', 'maxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. (default: 256)', 'minInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)', 'minInstancesPerNode: Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1. (default: 1)', 'numTrees: Number of trees to train (>= 1). (default: 20)', 'predictionCol: prediction column name. (default: prediction)', 'seed: random seed. (default: -5851613654371098793)', 'subsamplingRate: Fraction of the training data used for learning each decision tree, in range (0, 1]. (default: 1.0)']\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "# Build a RandomForestRegressor. The parameters that you need to define are labelCol (as before) and maxBins=40\n",
    "# You can read about maxBins using rf.explainParams().split('\\n')\n",
    "\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(labelCol=\"price\", maxBins=40)\n",
    "rf.explainParams().split('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressionModel (uid=RandomForestRegressor_4397bded437b6bf46466) with 20 trees\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "# Fit the model to the dataframe with the vectorized features\n",
    "\n",
    "rfModel = rf.fit(vecTrainDF)\n",
    "rfModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressionModel (uid=RandomForestRegressor_4397bded437b6bf46466) with 2 trees\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "# Cross Validation \n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "params = (ParamGridBuilder()\n",
    "          .addGrid(rf.numTrees, [1, 2])\n",
    "          .build())\n",
    "evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "cv = CrossValidator(estimator=rf, evaluator=evaluator, estimatorParamMaps=params, numFolds=10, seed=11)\n",
    "cvModel = cv.fit(vecTrainDF)\n",
    "cvModel.avgMetrics\n",
    "cvModel.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+--------------------+-----+---------------+\n|accommodates|bathrooms|review_scores_rating|price|       features|\n+------------+---------+--------------------+-----+---------------+\n|           4|      2.0|                  98|  350| [4.0,2.0,98.0]|\n|           2|      2.0|                 100|  250|[2.0,2.0,100.0]|\n|           6|      1.0|                  99|  275| [6.0,1.0,99.0]|\n|           2|      1.0|                  94|  115| [2.0,1.0,94.0]|\n|           8|      1.0|                  94|  145| [8.0,1.0,94.0]|\n|           2|      1.0|                  98|  135| [2.0,1.0,98.0]|\n|           1|      1.5|                  98|   67| [1.0,1.5,98.0]|\n|           2|      1.0|                 100|  141|[2.0,1.0,100.0]|\n|           2|      1.0|                  87|  157| [2.0,1.0,87.0]|\n|           2|      3.0|                  82|   50| [2.0,3.0,82.0]|\n+------------+---------+--------------------+-----+---------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "# Prepare test frame\n",
    "\n",
    "testDF = testDF.select(cols)\n",
    "vecTestDF = vecAssembler.transform(testDF)\n",
    "vecTestDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+--------------------+-----+---------------+------------------+\n|accommodates|bathrooms|review_scores_rating|price|       features|        prediction|\n+------------+---------+--------------------+-----+---------------+------------------+\n|           4|      2.0|                  98|  350| [4.0,2.0,98.0]|275.92905405405406|\n|           2|      2.0|                 100|  250|[2.0,2.0,100.0]|193.79044117647058|\n|           6|      1.0|                  99|  275| [6.0,1.0,99.0]| 336.6859077716865|\n|           2|      1.0|                  94|  115| [2.0,1.0,94.0]| 141.5931572386375|\n|           8|      1.0|                  94|  145| [8.0,1.0,94.0]| 195.0746440354228|\n|           2|      1.0|                  98|  135| [2.0,1.0,98.0]|156.73839694917575|\n|           1|      1.5|                  98|   67| [1.0,1.5,98.0]| 130.6401515151515|\n|           2|      1.0|                 100|  141|[2.0,1.0,100.0]|162.26366279069765|\n|           2|      1.0|                  87|  157| [2.0,1.0,87.0]| 141.5931572386375|\n|           2|      3.0|                  82|   50| [2.0,3.0,82.0]|106.96082089552239|\n|           2|      1.0|                  94|   45| [2.0,1.0,94.0]| 141.5931572386375|\n|           1|      1.0|                  92|   45| [1.0,1.0,92.0]| 141.5931572386375|\n|           2|      1.0|                 100|  165|[2.0,1.0,100.0]|162.26366279069765|\n|           4|      1.0|                  95|  195| [4.0,1.0,95.0]|198.88233634311513|\n|           3|      1.0|                  96|  143| [3.0,1.0,96.0]|162.81052783247685|\n|           3|      1.0|                  98|  150| [3.0,1.0,98.0]|174.93915452493331|\n|           6|      2.0|                  98|  425| [6.0,2.0,98.0]| 368.5801734570391|\n|           3|      1.0|                  89|  126| [3.0,1.0,89.0]|162.81052783247685|\n|           4|      1.0|                  97|  138| [4.0,1.0,97.0]|198.88233634311513|\n|           4|      1.0|                 100|  418|[4.0,1.0,100.0]|          200.2375|\n+------------+---------+--------------------+-----+---------------+------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "# Apply the cvModel to the test data (first you need to select the correct columns and vectorize them)\n",
    "\n",
    "predDF = cvModel.transform(vecTestDF)\n",
    "predDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+-----+------------------+\n|accommodates|       features|price|        prediction|\n+------------+---------------+-----+------------------+\n|           4| [4.0,2.0,98.0]|  350|303.69934110457683|\n|           2|[2.0,2.0,100.0]|  250|174.01891090100565|\n|           6| [6.0,1.0,99.0]|  275| 322.4187212630626|\n|           2| [2.0,1.0,94.0]|  115| 126.6057939396339|\n|           8| [8.0,1.0,94.0]|  145|238.99663866513734|\n|           2| [2.0,1.0,98.0]|  135|156.43191315337307|\n|           1| [1.0,1.5,98.0]|   67|167.21924844906218|\n|           2|[2.0,1.0,100.0]|  141|173.05749456837674|\n|           2| [2.0,1.0,87.0]|  157|124.09803193740763|\n|           2| [2.0,3.0,82.0]|   50| 65.20045977621189|\n+------------+---------------+-----+------------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "# ALTERNATIVE\n",
    "# Create a pipeline from all of the stages before (data preparation + model definition. Don't put cv in the pipeline.)\n",
    "# Apply the pipeline to test frame\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "input_cols =  ['accommodates', 'bathrooms', 'review_scores_rating']\n",
    "vecAssembler = VectorAssembler(inputCols=input_cols, outputCol=\"features\")\n",
    "rf = RandomForestRegressor(labelCol=\"price\")\n",
    "pipeline = Pipeline(stages=[vecAssembler, rf])\n",
    "\n",
    "pipelineModel = pipeline.fit(trainDF)\n",
    "\n",
    "predDF = pipelineModel.transform(testDF)\n",
    "predDF.select(\"accommodates\", \"features\", \"price\", \"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 of the random forest regression is: 0.532970919226\nRMSE of the random forest regression is: 103.130671995\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "# Evaluate the quality of the model using R2 and RMSE (which model are we evaluating?)\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "regressionEvaluator_r2 = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"price\", metricName=\"r2\")\n",
    "regressionEvaluator_rmse = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"price\", metricName=\"rmse\")\n",
    "\n",
    "r2 = regressionEvaluator_r2.evaluate(predDF)\n",
    "rmse = regressionEvaluator_rmse.evaluate(predDF)\n",
    "\n",
    "print 'R2 of the random forest regression is:', r2\n",
    "print 'RMSE of the random forest regression is:', rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 of the random forest regression is: 0.394755488148\nRMSE of the random forest regression is: 117.403573408\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "predDF_1 = cvModel.transform(vecTestDF)\n",
    "\n",
    "r2 = regressionEvaluator_r2.evaluate(predDF_1)\n",
    "rmse = regressionEvaluator_rmse.evaluate(predDF_1)\n",
    "\n",
    "print 'R2 of the random forest regression is:', r2\n",
    "print 'RMSE of the random forest regression is:', rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance for rfModel: (3,[0,1,2],[0.5759106280987623,0.28976990197264957,0.13431946992858818])\nFeature importance for cvModel: (3,[0,1,2],[0.5960604831032423,0.25322249301072597,0.15071702388603164])\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "# Which feature is the most important for our model?\n",
    "\n",
    "print 'Feature importance for rfModel:', rfModel.featureImportances\n",
    "print 'Feature importance for cvModel:', cvModel.bestModel.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                feature  importance\n0          accommodates    0.575911\n1             bathrooms    0.289770\n2  review_scores_rating    0.134319\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "import pandas as pd\n",
    "\n",
    "featuresDF = pd.DataFrame(list(zip(vecAssembler.getInputCols(), rfModel.featureImportances)), columns=[\"feature\", \"importance\"])\n",
    "featuresDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "%pyspark\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 2.0.0",
   "language": "python",
   "name": "spark2"
  },
  "language_info": {
   "codemirror_mode": "text/python",
   "file_extension": ".py",
   "mimetype": "text/python",
   "name": "scala",
   "pygments_lexer": "python",
   "version": "3.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
