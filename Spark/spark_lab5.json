{"paragraphs":[{"text":"%pyspark\nairbnbDF = (spark.read.format('csv')\n                    .option('header', 'true')\n                    .option('inferSchema', 'true')\n                    .option('sep', ',')\n                    .load('hdfs:///education/ece/big-data/2020/fall/bda/resources/lab5/airbnb_clean.csv'))\n\nairbnbDF.printSchema()","user":"l.cheong-ece","dateUpdated":"2020-11-27T11:33:05+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- host_is_superhost: string (nullable = true)\n |-- cancellation_policy: string (nullable = true)\n |-- instant_bookable: string (nullable = true)\n |-- host_total_listings_count: integer (nullable = true)\n |-- neighbourhood_cleansed: string (nullable = true)\n |-- latitude: double (nullable = true)\n |-- longitude: double (nullable = true)\n |-- property_type: string (nullable = true)\n |-- room_type: string (nullable = true)\n |-- accommodates: integer (nullable = true)\n |-- bathrooms: double (nullable = true)\n |-- bedrooms: integer (nullable = true)\n |-- beds: integer (nullable = true)\n |-- bed_type: string (nullable = true)\n |-- minimum_nights: integer (nullable = true)\n |-- number_of_reviews: integer (nullable = true)\n |-- review_scores_rating: integer (nullable = true)\n |-- review_scores_accuracy: integer (nullable = true)\n |-- review_scores_cleanliness: integer (nullable = true)\n |-- review_scores_checkin: integer (nullable = true)\n |-- review_scores_communication: integer (nullable = true)\n |-- review_scores_location: integer (nullable = true)\n |-- review_scores_value: integer (nullable = true)\n |-- price: integer (nullable = true)\n |-- bedrooms_na: integer (nullable = true)\n |-- bathrooms_na: integer (nullable = true)\n |-- beds_na: integer (nullable = true)\n |-- review_scores_rating_na: integer (nullable = true)\n |-- review_scores_accuracy_na: integer (nullable = true)\n |-- review_scores_cleanliness_na: integer (nullable = true)\n |-- review_scores_checkin_na: integer (nullable = true)\n |-- review_scores_communication_na: integer (nullable = true)\n |-- review_scores_location_na: integer (nullable = true)\n |-- review_scores_value_na: integer (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1606476785566_-703270176","id":"20201126-170419_197272242","dateCreated":"2020-11-27T11:33:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:37539"},{"text":"%md\n## Demo: Linear Regression","user":"l.cheong-ece","dateUpdated":"2020-11-27T11:33:05+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Demo: Linear Regression</h2>\n"}]},"apps":[],"jobName":"paragraph_1606476785588_1783657705","id":"20201126-181848_1981081982","dateCreated":"2020-11-27T11:33:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:37540"},{"text":"%pyspark\n# Split dataset to train and test\n\ntrainDF, testDF = airbnbDF.randomSplit([.8, .2], seed=123)\nprint 'No. of rows in train DF:', trainDF.count()\nprint 'No. of rows in test DF:', testDF.count()","user":"l.cheong-ece","dateUpdated":"2020-11-27T11:33:05+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"No. of rows in train DF: 802\nNo. of rows in test DF: 198\n"}]},"apps":[],"jobName":"paragraph_1606476785599_1914331117","id":"20201126-181455_1079104048","dateCreated":"2020-11-27T11:33:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:37541"},{"text":"%pyspark\n# We will try to predict the price in function of how many people can stay at the property\n\ntrainDF.select(\"price\", \"accommodates\").show()","user":"l.cheong-ece","dateUpdated":"2020-11-27T11:33:05+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-----+------------+\n|price|accommodates|\n+-----+------------+\n|  130|           3|\n|  250|           4|\n|  405|           6|\n|  185|           2|\n|  135|           2|\n|  150|           3|\n|  155|           4|\n|  160|           1|\n| 2281|           2|\n|  297|           2|\n|   81|           4|\n|   76|           5|\n|  155|           2|\n|  150|           2|\n|   75|           2|\n|  162|           2|\n|  200|           3|\n|  115|           2|\n|  125|           2|\n|  159|           3|\n+-----+------------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1606476785608_-358970049","id":"20201126-170626_1943173513","dateCreated":"2020-11-27T11:33:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:37542"},{"text":"%pyspark\n# We can first summarize the DF to see if it's really clean\n\ntrainDF.select(\"price\", \"accommodates\").summary().show()","user":"l.cheong-ece","dateUpdated":"2020-11-27T11:33:05+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------+------------------+------------------+\n|summary|             price|      accommodates|\n+-------+------------------+------------------+\n|  count|               802|               802|\n|   mean|213.15960099750623|3.2768079800498753|\n| stddev|203.85681852899708| 1.990786715309765|\n|    min|                29|                 1|\n|    25%|               100|                 2|\n|    50%|               150|                 2|\n|    75%|               249|                 4|\n|    max|              2281|                15|\n+-------+------------------+------------------+\n\n"}]},"apps":[],"jobName":"paragraph_1606476785616_-170319670","id":"20201126-181347_749775112","dateCreated":"2020-11-27T11:33:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:37543"},{"text":"%pyspark\n# Define linear regression estimator and fit the model\n# Why doesn't it work?\n\nfrom pyspark.ml.regression import LinearRegression\n\nlr = LinearRegression(featuresCol=\"accommodates\", labelCol=\"price\")\n#lrModel = lr.fit(trainDF)","user":"l.cheong-ece","dateUpdated":"2020-11-27T11:33:05+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Fail to execute line 7: lrModel = lr.fit(trainDF)\nTraceback (most recent call last):\n  File \"/data/4/yarn/local/usercache/petra/appcache/application_1602801542108_0674/container_e49_1602801542108_0674_01_000001/tmp/zeppelin_pyspark-3163550974154358337.py\", line 380, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 7, in <module>\n  File \"/data/4/yarn/local/usercache/petra/appcache/application_1602801542108_0674/container_e49_1602801542108_0674_01_000001/pyspark.zip/pyspark/ml/base.py\", line 132, in fit\n    return self._fit(dataset)\n  File \"/data/4/yarn/local/usercache/petra/appcache/application_1602801542108_0674/container_e49_1602801542108_0674_01_000001/pyspark.zip/pyspark/ml/wrapper.py\", line 288, in _fit\n    java_model = self._fit_java(dataset)\n  File \"/data/4/yarn/local/usercache/petra/appcache/application_1602801542108_0674/container_e49_1602801542108_0674_01_000001/pyspark.zip/pyspark/ml/wrapper.py\", line 285, in _fit_java\n    return self._java_obj.fit(dataset._jdf)\n  File \"/data/4/yarn/local/usercache/petra/appcache/application_1602801542108_0674/container_e49_1602801542108_0674_01_000001/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/data/4/yarn/local/usercache/petra/appcache/application_1602801542108_0674/container_e49_1602801542108_0674_01_000001/pyspark.zip/pyspark/sql/utils.py\", line 79, in deco\n    raise IllegalArgumentException(s.split(': ', 1)[1], stackTrace)\nIllegalArgumentException: u'requirement failed: Column accommodates must be of type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 but was actually IntegerType.'\n"}]},"apps":[],"jobName":"paragraph_1606476785624_511379033","id":"20201126-182148_561057068","dateCreated":"2020-11-27T11:33:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:37544"},{"text":"%pyspark\n# Vectorize the column 'accommodates'\n# Pay attention to .transform()\n\nfrom pyspark.ml.feature import VectorAssembler\n\nvecAssembler = VectorAssembler(inputCols=[\"accommodates\"], outputCol=\"features\")\nvecTrainDF = vecAssembler.transform(trainDF)\nvecTrainDF.select([\"accommodates\", \"features\"]).show()","user":"l.cheong-ece","dateUpdated":"2020-11-27T11:33:05+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------------+--------+\n|accommodates|features|\n+------------+--------+\n|           3|   [3.0]|\n|           4|   [4.0]|\n|           6|   [6.0]|\n|           2|   [2.0]|\n|           2|   [2.0]|\n|           3|   [3.0]|\n|           4|   [4.0]|\n|           1|   [1.0]|\n|           2|   [2.0]|\n|           2|   [2.0]|\n|           4|   [4.0]|\n|           5|   [5.0]|\n|           2|   [2.0]|\n|           2|   [2.0]|\n|           2|   [2.0]|\n|           2|   [2.0]|\n|           3|   [3.0]|\n|           2|   [2.0]|\n|           2|   [2.0]|\n|           3|   [3.0]|\n+------------+--------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1606476785632_-285840427","id":"20201126-182315_1653835585","dateCreated":"2020-11-27T11:33:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:37545"},{"text":"%pyspark\n# Let's try with two input columns\n\nvecAssembler_2 = VectorAssembler(inputCols=[\"accommodates\", \"bedrooms\"], outputCol=\"features\")\nvecTrainDF_2 = vecAssembler_2.transform(trainDF)\nvecTrainDF_2.select([\"accommodates\", \"bedrooms\", \"features\"]).show()","user":"l.cheong-ece","dateUpdated":"2020-11-27T11:33:05+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------------+--------+---------+\n|accommodates|bedrooms| features|\n+------------+--------+---------+\n|           3|       1|[3.0,1.0]|\n|           4|       2|[4.0,2.0]|\n|           6|       3|[6.0,3.0]|\n|           2|       1|[2.0,1.0]|\n|           2|       1|[2.0,1.0]|\n|           3|       1|[3.0,1.0]|\n|           4|       1|[4.0,1.0]|\n|           1|       1|[1.0,1.0]|\n|           2|       1|[2.0,1.0]|\n|           2|       1|[2.0,1.0]|\n|           4|       1|[4.0,1.0]|\n|           5|       2|[5.0,2.0]|\n|           2|       1|[2.0,1.0]|\n|           2|       1|[2.0,1.0]|\n|           2|       1|[2.0,1.0]|\n|           2|       1|[2.0,1.0]|\n|           3|       1|[3.0,1.0]|\n|           2|       1|[2.0,1.0]|\n|           2|       1|[2.0,1.0]|\n|           3|       1|[3.0,1.0]|\n+------------+--------+---------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1606476785639_917778334","id":"20201126-182349_626293401","dateCreated":"2020-11-27T11:33:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:37546"},{"text":"%pyspark\n# Which parameters can the estimator take?\n\nfrom pyspark.ml.regression import LinearRegression\n\nlr = LinearRegression(featuresCol=\"features\", labelCol=\"price\")\nlr.explainParams().split('\\n')\n","user":"l.cheong-ece","dateUpdated":"2020-11-27T11:33:05+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"['aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)', 'elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)', 'epsilon: The shape parameter to control the amount of robustness. Must be > 1.0. Only valid when loss is huber (default: 1.35)', 'featuresCol: features column name. (default: features, current: features)', 'fitIntercept: whether to fit an intercept term. (default: True)', 'labelCol: label column name. (default: label, current: price)', 'loss: The loss function to be optimized. Supported options: squaredError, huber. (default: squaredError)', 'maxIter: max number of iterations (>= 0). (default: 100)', 'predictionCol: prediction column name. (default: prediction)', 'regParam: regularization parameter (>= 0). (default: 0.0)', 'solver: The solver algorithm for optimization. Supported options: auto, normal, l-bfgs. (default: auto)', 'standardization: whether to standardize the training features before fitting the model. (default: True)', 'tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)', 'weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)']\n"}]},"apps":[],"jobName":"paragraph_1606476785647_-1197048375","id":"20201126-182413_1229901838","dateCreated":"2020-11-27T11:33:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:37547"},{"text":"%pyspark\n# Fit the estimator to the data\n\nlrModel = lr.fit(vecTrainDF)\nlrModel","user":"l.cheong-ece","dateUpdated":"2020-11-27T11:33:05+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"LinearRegression_4d59b3f133110b6f7a88\n"}]},"apps":[],"jobName":"paragraph_1606476785654_-63843498","id":"20201126-182501_1064629175","dateCreated":"2020-11-27T11:33:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:37548"},{"text":"%pyspark\n# Print the coefficients of the model\n\nk = lrModel.coefficients[0]\nn = lrModel.intercept\n\nprint 'y =', k, '* x +', n","user":"l.cheong-ece","dateUpdated":"2020-11-27T11:33:05+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"y = 62.627665173598565 * x + 7.94076798477\n"}]},"apps":[],"jobName":"paragraph_1606476785663_-877553523","id":"20201126-202355_1466708218","dateCreated":"2020-11-27T11:33:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:37549"},{"text":"%pyspark\n# Perform the cross validation (and optionally grid search)\n\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml import Pipeline\n\n# Define the params for cv (and grid search: commented line)\nparams = (ParamGridBuilder()\n          #.addGrid(lr.elasticNetParam, [0.0, 1.0])\n          .build())\nevaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"r2\")\n\ncv = CrossValidator(estimator=lr, evaluator=evaluator, estimatorParamMaps=params, numFolds=10, seed=11)\ncvModel = cv.fit(vecTrainDF)\n\nprint 'Avg. metrics:', cvModel.avgMetrics\nprint 'Best model slope:', cvModel.bestModel.coefficients[0]\nprint 'Best model intercept:', cvModel.bestModel.intercept","user":"l.cheong-ece","dateUpdated":"2020-11-27T11:33:05+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Avg. mertics: [0.3780198471330412]\nBest model slope: 62.627665173598565\nBest model intercept: 7.94076798477\n"}]},"apps":[],"jobName":"paragraph_1606476785670_1526641171","id":"20201126-202613_419045312","dateCreated":"2020-11-27T11:33:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:37550"},{"text":"%pyspark\n# Apply the model to the test set. But, first we need to process the test data the same way we processed the train data.\n\nvecTestDF = vecAssembler.transform(testDF)\npredDF = cvModel.transform(vecTestDF)\n\npredDF.select(\"accommodates\", \"features\", \"price\", \"prediction\").show()","user":"l.cheong-ece","dateUpdated":"2020-11-27T11:33:05+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------------+--------+-----+------------------+\n|accommodates|features|price|        prediction|\n+------------+--------+-----+------------------+\n|           4|   [4.0]|  350|  258.451428679161|\n|           2|   [2.0]|  250|133.19609833196387|\n|           6|   [6.0]|  275|383.70675902635816|\n|           2|   [2.0]|  115|133.19609833196387|\n|           8|   [8.0]|  145| 508.9620893735553|\n|           2|   [2.0]|  135|133.19609833196387|\n|           1|   [1.0]|   67| 70.56843315836531|\n|           2|   [2.0]|  141|133.19609833196387|\n|           2|   [2.0]|  157|133.19609833196387|\n|           2|   [2.0]|   50|133.19609833196387|\n|           2|   [2.0]|   45|133.19609833196387|\n|           1|   [1.0]|   45| 70.56843315836531|\n|           2|   [2.0]|  165|133.19609833196387|\n|           4|   [4.0]|  195|  258.451428679161|\n|           3|   [3.0]|  143|195.82376350556245|\n|           3|   [3.0]|  150|195.82376350556245|\n|           6|   [6.0]|  425|383.70675902635816|\n|           3|   [3.0]|  126|195.82376350556245|\n|           4|   [4.0]|  138|  258.451428679161|\n|           4|   [4.0]|  418|  258.451428679161|\n+------------+--------+-----+------------------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1606476785677_1957224152","id":"20201126-203244_197789648","dateCreated":"2020-11-27T11:33:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:37551"},{"text":"%pyspark\n# ALTERNATIVE\n# Create a pipeline from all of the stages before (data preparation + model definition. Don't put cv in the pipeline.)\n# Apply the pipeline to test frame\n\nfrom pyspark.ml import Pipeline\n\nvecAssembler = VectorAssembler(inputCols=[\"accommodates\"], outputCol=\"features\")\nlr = LinearRegression(featuresCol=\"features\", labelCol=\"price\")\npipeline = Pipeline(stages=[vecAssembler, lr])\n\npipelineModel = pipeline.fit(trainDF)\n\npredDF = pipelineModel.transform(testDF)\npredDF.select(\"accommodates\", \"features\", \"price\", \"prediction\").show()","user":"l.cheong-ece","dateUpdated":"2020-11-27T11:33:05+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------------+--------+-----+------------------+\n|accommodates|features|price|        prediction|\n+------------+--------+-----+------------------+\n|           4|   [4.0]|  350|  258.451428679161|\n|           2|   [2.0]|  250|133.19609833196387|\n|           6|   [6.0]|  275|383.70675902635816|\n|           2|   [2.0]|  115|133.19609833196387|\n|           8|   [8.0]|  145| 508.9620893735553|\n|           2|   [2.0]|  135|133.19609833196387|\n|           1|   [1.0]|   67| 70.56843315836531|\n|           2|   [2.0]|  141|133.19609833196387|\n|           2|   [2.0]|  157|133.19609833196387|\n|           2|   [2.0]|   50|133.19609833196387|\n|           2|   [2.0]|   45|133.19609833196387|\n|           1|   [1.0]|   45| 70.56843315836531|\n|           2|   [2.0]|  165|133.19609833196387|\n|           4|   [4.0]|  195|  258.451428679161|\n|           3|   [3.0]|  143|195.82376350556245|\n|           3|   [3.0]|  150|195.82376350556245|\n|           6|   [6.0]|  425|383.70675902635816|\n|           3|   [3.0]|  126|195.82376350556245|\n|           4|   [4.0]|  138|  258.451428679161|\n|           4|   [4.0]|  418|  258.451428679161|\n+------------+--------+-----+------------------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1606476785684_-1810285267","id":"20201126-203817_1522077330","dateCreated":"2020-11-27T11:33:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:37552"},{"text":"%pyspark\n# How good is the model?\n# RegressionEvaluator will compute the differences between the predicted price and the real one and calculate the metric of overall model quality (R2)\n\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nregressionEvaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"price\", metricName=\"r2\")\n\nr2 = regressionEvaluator.evaluate(predDF)\nprint 'R2 of the linear model is:', r2","user":"l.cheong-ece","dateUpdated":"2020-11-27T11:33:05+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"R2 of the linear model is: 0.427506233551\n"}]},"apps":[],"jobName":"paragraph_1606476785691_-886315495","id":"20201126-203845_548088799","dateCreated":"2020-11-27T11:33:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:37553"},{"text":"%md\n## Lab: Random Forest Regression\nBuild and evaluate a Random Forest model the same way we did with Linear Regression.\n","user":"l.cheong-ece","dateUpdated":"2020-11-27T11:33:05+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Lab: Random Forest Regression</h2>\n<p>Build and evaluate a Random Forest model the same way we did with Linear Regression.</p>\n"}]},"apps":[],"jobName":"paragraph_1606476785697_-1743432924","id":"20201126-203931_472487826","dateCreated":"2020-11-27T11:33:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:37554"},{"text":"%pyspark\n# Spit data to train and test set. Use the same split and seed as for linear regression, so we can compare the models at the end.\n\ntrainDF, testDF = airbnbDF.randomSplit([.8, .2], seed=123)\nprint 'No. of rows in train DF:', trainDF.count()\nprint 'No. of rows in test DF:', testDF.count()\n","user":"l.cheong-ece","dateUpdated":"2020-11-27T11:33:05+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"<console>:5: error: object zeppelin is not a member of package org.apache\n  var value: org.apache.zeppelin.spark.SparkZeppelinContext = _\n                        ^\n<console>:6: error: object zeppelin is not a member of package org.apache\n  def set(x: Any) = value = x.asInstanceOf[org.apache.zeppelin.spark.SparkZeppelinContext]\n                                                      ^\n"},{"type":"TEXT","data":"Fail to execute line 3: trainDF, testDF = airbnbDF.randomSplit([.8, .2], seed=123)\nTraceback (most recent call last):\n  File \"/data/2/yarn/local/usercache/petra/appcache/application_1602801542108_0694/container_e49_1602801542108_0694_01_000001/tmp/zeppelin_pyspark-2650058678434576849.py\", line 375, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 3, in <module>\nNameError: name 'airbnbDF' is not defined\n"}]},"apps":[],"jobName":"paragraph_1606476785704_-392927001","id":"20201126-204307_897634400","dateCreated":"2020-11-27T11:33:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:37555"},{"text":"%pyspark\n# Select only three atributes, since we didn't have time to look at the one-hot encoding - accommodates, bathrooms, review_scores_rating. \n# Predict the price as before\n# Name the dataframe trainDF \n\ncols =  ['accommodates', 'bathrooms', 'review_scores_rating', 'price']\ntrainDF = trainDF.select(cols)\ntrainDF.show(10)\n","user":"l.cheong-ece","dateUpdated":"2020-11-27T11:33:05+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Fail to execute line 6: trainDF = trainDF.select(cols)\nTraceback (most recent call last):\n  File \"/data/2/yarn/local/usercache/petra/appcache/application_1602801542108_0694/container_e49_1602801542108_0694_01_000001/tmp/zeppelin_pyspark-2650058678434576849.py\", line 375, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 6, in <module>\nNameError: name 'trainDF' is not defined\n"}]},"apps":[],"jobName":"paragraph_1606476785711_-1198506474","id":"20201126-204502_1643804267","dateCreated":"2020-11-27T11:33:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:37556"},{"text":"%pyspark\n# Vectorize the attribute (feature) columns with VectorAssembler\n\nfrom pyspark.ml.feature import VectorAssembler\n\ninput_cols =  ['accommodates', 'bathrooms', 'review_scores_rating']\nvecAssembler = VectorAssembler(inputCols=input_cols, outputCol=\"features\")\n\nvecTrainDF = vecAssembler.transform(trainDF)\nvecTrainDF.select([\"accommodates\", \"features\"]).show(10)\n","user":"l.cheong-ece","dateUpdated":"2020-11-27T11:33:05+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------------+---------------+\n|accommodates|       features|\n+------------+---------------+\n|           3| [3.0,1.0,88.0]|\n|           4| [4.0,2.0,74.0]|\n|           6|[6.0,2.5,100.0]|\n|           2| [2.0,1.0,96.0]|\n|           2| [2.0,1.0,95.0]|\n|           3| [3.0,1.0,92.0]|\n|           4| [4.0,1.0,99.0]|\n|           1| [1.0,1.0,80.0]|\n|           2| [2.0,1.0,99.0]|\n|           2| [2.0,1.0,97.0]|\n|           4| [4.0,1.0,98.0]|\n|           5| [5.0,1.0,93.0]|\n|           2| [2.0,1.0,99.0]|\n|           2| [2.0,1.0,93.0]|\n|           2| [2.0,2.0,91.0]|\n|           2| [2.0,1.0,98.0]|\n|           3| [3.0,1.0,76.0]|\n|           2| [2.0,1.0,98.0]|\n|           2| [2.0,1.0,89.0]|\n|           3| [3.0,1.0,94.0]|\n+------------+---------------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1606476785718_-1929952301","id":"20201126-205159_1631051026","dateCreated":"2020-11-27T11:33:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:37557"},{"text":"%pyspark\n# Build a RandomForestRegressor. The parameters that you need to define are labelCol (as before) and maxBins=40\n# You can read about maxBins using rf.explainParams().split('\\n')\n\nfrom pyspark.ml.regression import RandomForestRegressor\n\nrf = RandomForestRegressor(labelCol=\"price\", maxBins=40)\nrf.explainParams().split('\\n')\n","user":"l.cheong-ece","dateUpdated":"2020-11-27T11:33:05+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"['cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval. (default: False)', 'checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)', 'featureSubsetStrategy: The number of features to consider for splits at each tree node. Supported options: auto, all, onethird, sqrt, log2, (0.0-1.0], [1-n]. (default: auto)', 'featuresCol: features column name. (default: features)', 'impurity: Criterion used for information gain calculation (case-insensitive). Supported options: variance (default: variance)', 'labelCol: label column name. (default: label, current: price)', 'maxBins: Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature. (default: 32, current: 40)', 'maxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. (default: 5)', 'maxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. (default: 256)', 'minInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)', 'minInstancesPerNode: Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1. (default: 1)', 'numTrees: Number of trees to train (>= 1). (default: 20)', 'predictionCol: prediction column name. (default: prediction)', 'seed: random seed. (default: -5851613654371098793)', 'subsamplingRate: Fraction of the training data used for learning each decision tree, in range (0, 1]. (default: 1.0)']\n"}]},"apps":[],"jobName":"paragraph_1606476785725_1608616849","id":"20201126-205336_11415105","dateCreated":"2020-11-27T11:33:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:37558"},{"text":"%pyspark\n# Fit the model to the dataframe with the vectorized features\n\nrfModel = rf.fit(vecTrainDF)\nrfModel\n","user":"l.cheong-ece","dateUpdated":"2020-11-27T11:33:05+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"RandomForestRegressionModel (uid=RandomForestRegressor_4397bded437b6bf46466) with 20 trees\n"}]},"apps":[],"jobName":"paragraph_1606476785732_-836945933","id":"20201126-205411_1872242433","dateCreated":"2020-11-27T11:33:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:37559"},{"text":"%pyspark\n# Cross Validation \n\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n\nparams = (ParamGridBuilder()\n          .addGrid(rf.numTrees, [1, 2])\n          .build())\nevaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"r2\")\n\ncv = CrossValidator(estimator=rf, evaluator=evaluator, estimatorParamMaps=params, numFolds=10, seed=11)\ncvModel = cv.fit(vecTrainDF)\ncvModel.avgMetrics\ncvModel.bestModel","user":"l.cheong-ece","dateUpdated":"2020-11-27T11:33:05+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"RandomForestRegressionModel (uid=RandomForestRegressor_4397bded437b6bf46466) with 2 trees\n"}]},"apps":[],"jobName":"paragraph_1606476785739_-1686860582","id":"20201126-205424_1809911808","dateCreated":"2020-11-27T11:33:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:37560"},{"text":"%pyspark\n# Prepare test frame\n\ntestDF = testDF.select(cols)\nvecTestDF = vecAssembler.transform(testDF)\nvecTestDF.show(10)","user":"l.cheong-ece","dateUpdated":"2020-11-27T11:33:05+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------------+---------+--------------------+-----+---------------+\n|accommodates|bathrooms|review_scores_rating|price|       features|\n+------------+---------+--------------------+-----+---------------+\n|           4|      2.0|                  98|  350| [4.0,2.0,98.0]|\n|           2|      2.0|                 100|  250|[2.0,2.0,100.0]|\n|           6|      1.0|                  99|  275| [6.0,1.0,99.0]|\n|           2|      1.0|                  94|  115| [2.0,1.0,94.0]|\n|           8|      1.0|                  94|  145| [8.0,1.0,94.0]|\n|           2|      1.0|                  98|  135| [2.0,1.0,98.0]|\n|           1|      1.5|                  98|   67| [1.0,1.5,98.0]|\n|           2|      1.0|                 100|  141|[2.0,1.0,100.0]|\n|           2|      1.0|                  87|  157| [2.0,1.0,87.0]|\n|           2|      3.0|                  82|   50| [2.0,3.0,82.0]|\n+------------+---------+--------------------+-----+---------------+\nonly showing top 10 rows\n\n"}]},"apps":[],"jobName":"paragraph_1606476785747_-1655731376","id":"20201126-205437_1160801103","dateCreated":"2020-11-27T11:33:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:37561"},{"text":"%pyspark\n# Apply the cvModel to the test data (first you need to select the correct columns and vectorize them)\n\npredDF = cvModel.transform(vecTestDF)\npredDF.show()","user":"l.cheong-ece","dateUpdated":"2020-11-27T11:33:05+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------------+---------+--------------------+-----+---------------+------------------+\n|accommodates|bathrooms|review_scores_rating|price|       features|        prediction|\n+------------+---------+--------------------+-----+---------------+------------------+\n|           4|      2.0|                  98|  350| [4.0,2.0,98.0]|275.92905405405406|\n|           2|      2.0|                 100|  250|[2.0,2.0,100.0]|193.79044117647058|\n|           6|      1.0|                  99|  275| [6.0,1.0,99.0]| 336.6859077716865|\n|           2|      1.0|                  94|  115| [2.0,1.0,94.0]| 141.5931572386375|\n|           8|      1.0|                  94|  145| [8.0,1.0,94.0]| 195.0746440354228|\n|           2|      1.0|                  98|  135| [2.0,1.0,98.0]|156.73839694917575|\n|           1|      1.5|                  98|   67| [1.0,1.5,98.0]| 130.6401515151515|\n|           2|      1.0|                 100|  141|[2.0,1.0,100.0]|162.26366279069765|\n|           2|      1.0|                  87|  157| [2.0,1.0,87.0]| 141.5931572386375|\n|           2|      3.0|                  82|   50| [2.0,3.0,82.0]|106.96082089552239|\n|           2|      1.0|                  94|   45| [2.0,1.0,94.0]| 141.5931572386375|\n|           1|      1.0|                  92|   45| [1.0,1.0,92.0]| 141.5931572386375|\n|           2|      1.0|                 100|  165|[2.0,1.0,100.0]|162.26366279069765|\n|           4|      1.0|                  95|  195| [4.0,1.0,95.0]|198.88233634311513|\n|           3|      1.0|                  96|  143| [3.0,1.0,96.0]|162.81052783247685|\n|           3|      1.0|                  98|  150| [3.0,1.0,98.0]|174.93915452493331|\n|           6|      2.0|                  98|  425| [6.0,2.0,98.0]| 368.5801734570391|\n|           3|      1.0|                  89|  126| [3.0,1.0,89.0]|162.81052783247685|\n|           4|      1.0|                  97|  138| [4.0,1.0,97.0]|198.88233634311513|\n|           4|      1.0|                 100|  418|[4.0,1.0,100.0]|          200.2375|\n+------------+---------+--------------------+-----+---------------+------------------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1606476785754_-842912736","id":"20201126-205744_1334925889","dateCreated":"2020-11-27T11:33:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:37562"},{"text":"%pyspark\n# ALTERNATIVE\n# Create a pipeline from all of the stages before (data preparation + model definition. Don't put cv in the pipeline.)\n# Apply the pipeline to test frame\n\nfrom pyspark.ml import Pipeline\n\ninput_cols =  ['accommodates', 'bathrooms', 'review_scores_rating']\nvecAssembler = VectorAssembler(inputCols=input_cols, outputCol=\"features\")\nrf = RandomForestRegressor(labelCol=\"price\")\npipeline = Pipeline(stages=[vecAssembler, rf])\n\npipelineModel = pipeline.fit(trainDF)\n\npredDF = pipelineModel.transform(testDF)\npredDF.select(\"accommodates\", \"features\", \"price\", \"prediction\").show(10)","user":"l.cheong-ece","dateUpdated":"2020-11-27T11:33:05+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------------+---------------+-----+------------------+\n|accommodates|       features|price|        prediction|\n+------------+---------------+-----+------------------+\n|           4| [4.0,2.0,98.0]|  350|303.69934110457683|\n|           2|[2.0,2.0,100.0]|  250|174.01891090100565|\n|           6| [6.0,1.0,99.0]|  275| 322.4187212630626|\n|           2| [2.0,1.0,94.0]|  115| 126.6057939396339|\n|           8| [8.0,1.0,94.0]|  145|238.99663866513734|\n|           2| [2.0,1.0,98.0]|  135|156.43191315337307|\n|           1| [1.0,1.5,98.0]|   67|167.21924844906218|\n|           2|[2.0,1.0,100.0]|  141|173.05749456837674|\n|           2| [2.0,1.0,87.0]|  157|124.09803193740763|\n|           2| [2.0,3.0,82.0]|   50| 65.20045977621189|\n+------------+---------------+-----+------------------+\nonly showing top 10 rows\n\n"}]},"apps":[],"jobName":"paragraph_1606476785762_2017195951","id":"20201126-205809_2007677940","dateCreated":"2020-11-27T11:33:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:37563"},{"text":"%pyspark\n# Evaluate the quality of the model using R2 and RMSE (which model are we evaluating?)\n\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nregressionEvaluator_r2 = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"price\", metricName=\"r2\")\nregressionEvaluator_rmse = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"price\", metricName=\"rmse\")\n\nr2 = regressionEvaluator_r2.evaluate(predDF)\nrmse = regressionEvaluator_rmse.evaluate(predDF)\n\nprint 'R2 of the random forest regression is:', r2\nprint 'RMSE of the random forest regression is:', rmse","user":"l.cheong-ece","dateUpdated":"2020-11-27T11:33:05+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"R2 of the random forest regression is: 0.532970919226\nRMSE of the random forest regression is: 103.130671995\n"}]},"apps":[],"jobName":"paragraph_1606476785770_2062361494","id":"20201126-205842_255664236","dateCreated":"2020-11-27T11:33:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:37564"},{"text":"%pyspark\npredDF_1 = cvModel.transform(vecTestDF)\n\nr2 = regressionEvaluator_r2.evaluate(predDF_1)\nrmse = regressionEvaluator_rmse.evaluate(predDF_1)\n\nprint 'R2 of the random forest regression is:', r2\nprint 'RMSE of the random forest regression is:', rmse","user":"l.cheong-ece","dateUpdated":"2020-11-27T11:33:05+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"R2 of the random forest regression is: 0.394755488148\nRMSE of the random forest regression is: 117.403573408\n"}]},"apps":[],"jobName":"paragraph_1606476785779_1821349877","id":"20201127-105542_1757985858","dateCreated":"2020-11-27T11:33:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:37565"},{"text":"%pyspark\n# Which feature is the most important for our model?\n\nprint 'Feature importance for rfModel:', rfModel.featureImportances\nprint 'Feature importance for cvModel:', cvModel.bestModel.featureImportances","user":"l.cheong-ece","dateUpdated":"2020-11-27T11:33:05+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Feature importance for rfModel: (3,[0,1,2],[0.5759106280987623,0.28976990197264957,0.13431946992858818])\nFeature importance for cvModel: (3,[0,1,2],[0.5960604831032423,0.25322249301072597,0.15071702388603164])\n"}]},"apps":[],"jobName":"paragraph_1606476785787_242916262","id":"20201126-210220_107745704","dateCreated":"2020-11-27T11:33:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:37566"},{"text":"%pyspark\nimport pandas as pd\n\nfeaturesDF = pd.DataFrame(list(zip(vecAssembler.getInputCols(), rfModel.featureImportances)), columns=[\"feature\", \"importance\"])\nfeaturesDF","user":"l.cheong-ece","dateUpdated":"2020-11-27T11:33:05+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"                feature  importance\n0          accommodates    0.575911\n1             bathrooms    0.289770\n2  review_scores_rating    0.134319\n"}]},"apps":[],"jobName":"paragraph_1606476785794_-854026162","id":"20201126-210308_1564791791","dateCreated":"2020-11-27T11:33:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:37567"},{"text":"%pyspark\n","user":"l.cheong-ece","dateUpdated":"2020-11-27T11:33:05+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1606476785802_1313249647","id":"20201126-210336_1731341385","dateCreated":"2020-11-27T11:33:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:37568"}],"name":"ece-2020/spark/l.cheong/lab5","id":"2FRGQ8GBV","noteParams":{},"noteForms":{},"angularObjects":{"spark2:l.cheong-ece:":[],"md:shared_process":[],"sh:shared_process":[],"livy:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}